  0%|                                                                                                                                                           | 0/2952 [00:00<?, ?it/s]Traceback (most recent call last):
./all_cropped_images\dr156_1_18.jpg
./all_cropped_images\dr173_2_3.jpg
./all_cropped_images\dr87_7_44.jpg
./all_cropped_images\dr38_18_9.jpg
./all_cropped_images\dr122_1_6.jpg
./all_cropped_images\dr87_7_13.jpg
./all_cropped_images\dr38_24_38.jpg
./all_cropped_images\dr43_4_18.jpg
./all_cropped_images\dr7_2_6.jpg
./all_cropped_images\dr109_1_7.jpg
./all_cropped_images\dr4_6_19.jpg
./all_cropped_images\dr75_3_9.jpg
./all_cropped_images\dr43_6_23.jpg
./all_cropped_images\dr69_11_15.jpg
./all_cropped_images\dr114_7_2.jpg
./all_cropped_images\dr112_2_5.jpg
  File "c:\Users\SCS\Downloads\Annotated\train.py", line 183, in <module>
    trainer.train()
  File "C:\Users\SCS\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\trainer.py", line 2171, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SCS\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\trainer.py", line 2531, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SCS\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\trainer.py", line 3675, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SCS\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\trainer.py", line 3731, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "C:\Users\SCS\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SCS\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SCS\AppData\Local\Programs\Python\Python312\Lib\site-packages\transformers\models\vision_encoder_decoder\modeling_vision_encoder_decoder.py", line 592, in forward
    encoder_outputs = self.encoder(
                      ^^^^^^^^^^^^^
  File "C:\Users\SCS\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\SCS\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: ViTModel.forward() got an unexpected keyword argument 'num_items_in_batch'
